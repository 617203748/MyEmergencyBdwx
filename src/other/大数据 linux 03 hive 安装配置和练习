											Hive的安装和使用
											
前提：
    在安装Hive之前，要求先：
	- 安装JDK 8
	- 安装Hadoop-2.2.0
	- 安装MySQL 5.6.34
	
零、安装Hbase
	将 hbase-1.2.5-bin.tar.gz 解压至 /mydata/setup
	# vim /etc/profile
		export HBASE_HOME=/mydata/setup/hbase-1.2.5
		export PATH=$HBASE_HOME/bin:$PATH
	
	# cd /mydata/setup/hbase-1.2.5/conf
	
	# vim hbase-env.sh
		export JAVA_HOME=/mydata/java/jdk1.8.0_112
		
	# mkdir /mydata/setup/hadoop-2.2.0/dfs/zookerper
	# vim hbase-site.xml
		<configuration>
		    <!-- 指定本机的hbase的存储目录 -->
	        <property>
	            <name>hbase.rootdir</name>
	            <value>hdfs://localhost:8020/hbase</value>
	        </property>
	        
	        <!-- 指定hbase的运行模式 true代表全分布模式-->
	        <property>
	            <name>hbase.cluster.distributed</name>
	            <value>true</value>
	        </property>
	
		    <!-- 
	        <property>
	            <name>hbase.zookerper.quorum</name>
	            <value>localhost</value>
	        </property>
	
	        <property>
	            <name>hbase.zookeeper.property.dataDir</name>
	            <value>/mydata/setup/hadoop-2.2.0/dfs/zookeeper</value>
	        </property>
	        -->
		</configuration>
	# start-hbase.sh
	# hbase version


一、安装
    1. 下载hive最新的稳定版本的包，并解压缩到用户主目录下：
        # tar -xzvf apache-hive-2.0.0-bin.tar.gz
        解压到
        /mydata/setup/

    2. 设置环境变量：
        # vim /etc/profile
       在文件的最后，添加如下两句：
		export  HIVE_HOME=/mydata/setup/hive-2.0.0
		export  PATH=$HIVE_HOME/bin:$PATH
       然后执行：
        # source /etc/profile

二、配置管理
    首先进入conf目录把所有带template后缀的文件移除后缀。
    其中hive-default.xml移除后缀后，需要修改名为hive-site.xml。

    1. 可通过以下方法操纵Hive配置：
       1.1 修改hive-env.sh
             $ cp hive-env.sh.template hive-env.sh

           因为Hive使用了 Hadoop, 需要在 hive-env.sh 文件中指定 Hadoop 安装路径：
           vim  hive-env.sh
	     export JAVA_HOME=/mydata/java/jdk1.8.0_112
	     export HADOOP_HOME=/mydata/setup/hadoop-2.2.0
	     export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
	     export HIVE_HOME=/mydata/setup/hive-2.0.0
	     export HIVE_CONF_DIR=$HIVE_HOME/conf
	     export HIVE_AUX_JARS_PATH=$HIVE_HOME/lib

       1.2 修改hive-log4j2.properties，配置hive的log
         # cp hive-log4j2.properties.template hive-log4j2.properties
	     # vim hive-log4j2.properties 
           配置下面的2个参数(如果没有logs目录，在hive根目录下创建它)：
	     property.hive.log.dir=/mydata/setup/hive-2.0.0/logs 
	     property.hive.log.file=hive.log

       1.3 修改hive-site.xml 
           在/mydata/setup/hive-2.0.0/下新建一个tmp目录，在tmp/下新建一个hduser目录。
         # mkdir /mydata/setup/hive-2.0.0/tmp
	     # mkdir /mydata/setup/hive-2.0.0/tmp/root

	     # cp hive-default.xml.template hive-site.xml
           将hive-site.xml文件中：
	     -- 凡是${system:java.io.tmpdir}都替换成:/mydata/setup/hive-2.0.0/tmp
             -- 凡是${system:user.name}都替换为root
           (参考hive-site2.xml)

       1.4 配置Hive Metastore
           默认情况下, Hive的元数据保存在了内嵌的derby数据库里, 但一般情况下生产环境使用MySQL来存放Hive元数据。
           1) 将 mysql-connector-java-5.1.40.jar 放入 $HIVE_HOME/lib 下。（mysql jdbc驱动程序）
           2) hive-site.xml 中配置 MySQL 数据库连接信息：
	      (参考hive-site2.xml  本文最末)
	      
	   1.5 初始化 metastore 的数据库
	       schematool -initSchema -dbType mysql
	       
	   1.6 运行
	   	   hive       

三、为Hive创建HDFS目录
    1. 必须先安装了Hadoop，并配置了HADOOP_HOME环境变量

    2. 在Hive中创建表之前，需要在HDFS上创建目录/tmp/hive和/user/hduser/warehouse，并给它们赋予写权限。
       在HDFS中将它们设为chmod 777，然后才可以在Hive中创建表:
        $ $HADOOP_HOME/bin/hdfs dfs -mkdir       /tmp
        $ $HADOOP_HOME/bin/hdfs dfs -mkdir       /tmp/hive
        $ $HADOOP_HOME/bin/hdfs dfs -mkdir       /user/hduser/warehouse
        //用户组加写权限
        $ $HADOOP_HOME/bin/hdfs dfs -chmod -R 777   /tmp
        $ $HADOOP_HOME/bin/hdfs dfs -chmod -R 777   /user/hduser/warehouse

四、运行
   1. 运行Hive CLI:
      在命令行运行hive命令时必须保证HDFS已经启动。可以使用start-dfs.sh来启动HDFS。 
      特别说明：从 Hive 2.1 版本开始, 在第一次运行hive之前，需要先运行schematool命令来执行初始化操作。
        $ schematool -initSchema -dbType derby

        // 如果是使用MySQL数据库：
        $ schematool -initSchema -dbType mysql
        执行成功后，可以查看MySQL中元数据库hive是否已经创建成功。

        // 进入hive命令行：
        $ $HIVE_HOME/bin/hive
   
        // 使用 show tables 来显示所有的表:
        hive> show tables;

	// 退出hive
	hive> quit;
	
	
五、练习
	-- 将本地文件传到HDFS上（主文件夹下）
	-- 在cloud01的终端窗口执行: 
	$ hdfs dfs -mkdir /hivedata
	$ hdfs dfs -put test2 /hivedata  
	或者 
	$ hdfs dfs -copyFromLocal test2 /hivedata
	
	-- 新建一张以“TAB键”分隔的表，名叫sample_hdfs
	hive> create table test_zzz(id int,name string) row format delimited fields terminated by '\t';
	
	-- 将以“tab键”分隔的hdfs文档数据加载到“test_zzz”表中
	-- hive会将/zzz/testdata.txt 移动到 /mydata/setup/hive-2.0.0/warehouse
	hive> load data inpath '/zzz/test2' into table test_zzz;
	
	-- 查询test_zzz表(如果出现null，说明表格式和数据格式不对应)
	hive> select * from test_zzz;
	
	-- 重新构建sample_hdfs表
	hive> truncate table test_zzz;
	hive> select * from test_zzz;


	
	
	
	
	
	
	
	
下面是配置	
	
<!-- 修改下面这些属性 -->
      <property>
	    <name>hive.exec.scratchdir</name>
	    <value>/tmp/hive</value>
	    <description>HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&lt;username&gt; is created, with ${hive.scratch.dir.permission}.</description>
      </property>

	  <!-- 001 -->
      <property>
	    <name>hive.exec.local.scratchdir</name>
	    <value>/mydata/setup/hive-2.0.0/tmp/root</value>
	    <description>Local scratch space for Hive jobs</description>
      </property>

	  <!-- 002 -->
      <property>
	    <name>hive.downloaded.resources.dir</name>
	    <value>/mydata/setup/hive-2.0.0/tmp/${hive.session.id}_resources</value>
	    <description>Temporary local directory for added resources in the remote file system.</description>
      </property>

	  <!-- 003 -->
      <property>
	    <name>hive.querylog.location</name>
	    <value>/mydata/setup/hive-2.0.0/tmp/root</value>
	    <description>Location of Hive run time structured log file</description>
      </property>  

	  <!-- 004 -->
	  <property>
	    <name>hive.aux.jars.path</name>
	    <value>/mydata/setup/hive-2.0.0/lib,/mydata/setup/hive-2.0.0/jdbc</value>
	    <description>These JAR file are available to all users for all jobs。</description>
      </property>


	  <!-- 005 -->
	  <property>
	    <name>hive.metastore.warehouse.dir</name>
	    <value>hdfs://localhost:9000/mydata/setup/hive-2.0.0/warehouse</value>
	  </property>
  
      <!--配置Hive Metastore-->
	  <!-- 006 -->
    <property>
	 <name>javax.jdo.option.ConnectionURL</name>
	 <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8</value>
    </property>

     <!-- 007 -->
    <property>
	 <name>javax.jdo.option.ConnectionDriverName</name>
	 <value>com.mysql.jdbc.Driver</value>
    </property>

	<!-- 008 -->
    <property>
	 <name>javax.jdo.option.ConnectionUserName</name>
	 <value>root</value>
    </property>

	<!-- 009 -->
    <property>
	 <name>javax.jdo.option.ConnectionPassword</name>
	 <value>root</value>
    </property>	  

<!-- 配置下面两个属性，可以配置 hive 2.x web ui -->
    <property>
        <name>hive.server2.webui.host</name>
        <value>localhost</value>
    </property>

    <property>
        <name>hive.server2.webui.port</name>
        <value>10002</value>
    </property>
<!-- 重启HiveServer2，访问http://localhost:10002/ -->


testdata.txt内容
	1	张三
	2	李四
	3	王老五



											
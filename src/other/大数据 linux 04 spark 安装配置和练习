spark  		大数据处理（基于内存 快）

mapreduce 	大数据处理（基于磁盘）

hadoop		大数据处理 + 大数据存储

					
在安装Hive之前，要求先：
- 安装JDK 8
					
												Spark 安装配置
一、安装Scala
	# tar –zvxf ~/scala-2.10-2.10.4.tgz
	# vim /etc/profile
		export SCALA_HOME=/mydata/scala/scala-2.10.4
		export PATH=$PATH:$SCALA_HOME/bin
	# source /etc/profile
	
	3.发送至slave1, slave2
	scp -r scala-2.10.4 cloud02:~/
	scp -r scala-2.10.4 cloud03:~/
	分别在cloud02, cloud03上重复步骤2
	4.在三个节点上分别验证
	scala –version
	
	
二、安装spark
	把安装包解压到 /mydata/setup 下
	# tar –zvxf spark-1.4.0-bin-hadoop2.4.tgz


2. 配置文件
	cd /mydata/setup/spark-1.4.0-bin-hadoop2.4/conf
	
	(1) 配置spark-env.sh
		# cp spark-env.sh.template spark-env.sh		
			# vim spark-env.sh	追加
			export HADOOP_CONF_DIR=/mydata/setup/hadoop-2.2.0
			export JAVA_HOME=/mydata/java/jdk1.8.0_112
			export SCALA_HOME=/mydata/scala/scala-2.10.4
			export SPARK_MASTER_IP=node0
			export SPARK_LOCAL_IP=node0
			export SPARK_MASTER_PORT=7077
			export SPARK_MASTER_WEBUI_PORT=8080
			export SPARK_WORKER_PORT=7078
			export SPARK_WORKER_WEBUI_PORT=8081
			export SPARK_WORKER_CORES=3
			export SPARK_WORKER_INSTANCES=4
			export SPARK_WORKER_MEMORY=4g
			export SPARK_JAR=/mydata/setup/spark-1.4.0-bin-hadoop2.4/lib/spark-assembly-1.4.0-hadoop2.4.0.jar	
	
	(2) 配置spark-defaults.conf
		# cp spark-defaults.conf.template spark-defaults.conf
		# vim spark-defaults.conf					追加
			spark.master=spark://node0:7077
	
	(3) 配置spark-slaves
		# cp slaves.template slaves					追加
			localhost
	
	3.配置环境变量      
		# vim gedit /etc/profile
			加入如下代码
			export SPARK_HOME=/mydata/setup/spark-1.4.0-bin-hadoop2.4/
			export PATH=$PATH:$SPARK_HOME/bin
			使文件生效：
		# source /etc/profile
	
	4. 发送至slave1, slave2
		scp -r /mydata/scala/scala-2.10.4 node1:~/
		scp -r /mydata/scala/scala-2.10.4 node2:~/
		
		scp -r /mydata/setup/spark-1.4.0-bin-hadoop2.4 node1:~/
		scp -r /mydata/setup/spark-1.4.0-bin-hadoop2.4 node2:~/
	分别在每个节点上重复步骤3
	
	5. 启动spark
		启动Hadoop
			cd /mydata/setup/spark-1.4.0-bin-hadoop2.4
		启动方式1：
			sbin/start-master.sh
			sbin/start-slaves.sh
		启动方式2：
			sbin/start-all.sh
			
		关闭方式：
			cd spark-1.4.0-bin-hadoop2.4
			sbin/stop-all.sh
		
		验证一
			# bin/spark-shell
		验证二
			打开网址 http://172.16.105.159:8080/



